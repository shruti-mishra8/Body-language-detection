{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01761fde",
   "metadata": {},
   "source": [
    "## 1. Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b875e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn mediapipe matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010b62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65171858",
   "metadata": {},
   "source": [
    "## 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b121de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities\n",
    "mp_holistic = mp.solutions.holistic #holistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d47950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) # making predictions\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67008d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a392b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        cv2.imshow(\"OpenCV feed\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5181b8f",
   "metadata": {},
   "source": [
    "## 3. Extract keypoint values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a341ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b170326",
   "metadata": {},
   "source": [
    "## 4. Setup folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c99dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_data')\n",
    "actions = np.array(['hello', 'peace', 'all the best', 'wakanda forever', 'dedicate your hearts'])\n",
    "no_sequences = 100\n",
    "sequence_length = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3ef4f",
   "metadata": {},
   "source": [
    " ## 5. Collect keypoint values for testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d37952",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                  \n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                image, results = mediapipe_detection(frame, holistic) # making detection\n",
    "\n",
    "                draw_styled_landmarks(image, results) # drawing landmarks\n",
    "                \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                               cv2.FONT_HERSHEY_COMPLEX, 2, (0,0,0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {action} video number {sequence}', (15,12),\n",
    "                               cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "                    cv2.waitKey(500)\n",
    "                else:\n",
    "                    cv2.putText(image, f'Collecting frames for {action} video number {sequence}', (15,12),\n",
    "                               cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                \n",
    "                # export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                                        \n",
    "                \n",
    "                cv2.imshow(\"OpenCV feed\", image) \n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87300f63",
   "metadata": {},
   "source": [
    "## 6. Preprocessing of data and label and feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da599178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a66695",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0455d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8352e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd41dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd47482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Compute the mean and standard deviation along the third dimension\n",
    "mean = np.mean(X_train, axis=2, keepdims=True)\n",
    "std = np.std(X_train, axis=2, keepdims=True)\n",
    "\n",
    "# Normalize the training data\n",
    "X_train_normalized = (X_train - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe136173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation along the third dimension\n",
    "mean_test = np.mean(X_test, axis=2, keepdims=True)\n",
    "std_test = np.std(X_test, axis=2, keepdims=True)\n",
    "\n",
    "# Normalize the test data using the mean and standard deviation of the test data\n",
    "X_test_normalized = (X_test - mean_test) / std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71506945",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540724f5",
   "metadata": {},
   "source": [
    "## 7. Build and train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be75425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76238456",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(40,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(32, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=700, callbacks=[tb_callback], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking is input has null values\n",
    "if not np.any(np.isnan(X_train)):\n",
    "    print(\"There are no NaN values in X_train\")\n",
    "else:\n",
    "    print(\"There are NaN values in X_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50631875",
   "metadata": {},
   "source": [
    "##  8. Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228958e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[17])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[17])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053f617",
   "metadata": {},
   "source": [
    "## 9. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f765d",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5603f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492b9a2",
   "metadata": {},
   "source": [
    "## 11. Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb22f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e573924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 40, 64)            442112    \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 40, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 40, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 40, 32)            12416     \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 609,221\n",
      "Trainable params: 609,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('action1.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (245,117,16), (245,117,16), (245,117,16), (245,117,16), (245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d13ddb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "dedicate your hearts\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "dedicate your hearts\n",
      "dedicate your hearts\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "dedicate your hearts\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n",
      "wakanda forever\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture('wakanda forever.mp4')\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "                \n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                        \n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a9695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
